### 🧠 지도학습 (Supervised Learning)

| 항목      | 설명                                |
| ------- | --------------------------------- |
| 정의      | 정답(label)이 있는 데이터를 사용하여 학습        |
| 입력      | 데이터와 해당 정답 (예: 이미지 + 고양이/강아지 라벨)  |
| 목적      | 새로운 데이터에 대한 예측 모델 만들기             |
| 주요 알고리즘 | 선형회귀, 로지스틱 회귀, 의사결정나무, SVM, 신경망 등 |
| 예시      | 스팸 메일 분류, 주가 예측, 질병 진단 등          |

---

### 🔍 비지도학습 (Unsupervised Learning)

| 항목      | 설명                                   |
| ------- | ------------------------------------ |
| 정의      | 정답(label) 없이 데이터만 가지고 학습             |
| 입력      | 데이터만 존재, 라벨 없음                       |
| 목적      | 데이터의 숨겨진 구조나 패턴 찾기                   |
| 주요 알고리즘 | 군집화(K-means), 차원 축소(PCA), 연관 규칙 학습 등 |
| 예시      | 고객 세분화, 이상 탐지, 문서 주제 분류 등            |

---

### 🎮 강화학습 (Reinforcement Learning)

| 항목      | 설명                                 |
| ------- | ---------------------------------- |
| 정의      | 에이전트가 환경과 상호작용하며 보상(reward)을 통해 학습 |
| 입력      | 상태(state), 행동(action), 보상(reward)  |
| 목적      | 최대 보상을 얻는 정책(policy) 학습            |
| 주요 알고리즘 | Q-learning, DQN, PPO, SARSA 등      |
| 예시      | 게임 AI, 로봇 제어, 추천 시스템 최적화 등         |

---

### 🧭 비교 요약

| 기준     | 지도학습   | 비지도학습     | 강화학습        |
| ------ | ------ | --------- | ----------- |
| 데이터 라벨 | 있음     | 없음        | 환경으로부터 받음   |
| 학습 대상  | 예측 함수  | 데이터 구조    | 행동 전략       |
| 출력 유형  | 정해진 정답 | 군집 또는 패턴  | 최적의 행동      |
| 적용 분야  | 분류, 회귀 | 군집화, 차원축소 | 게임, 로봇, 최적화 |