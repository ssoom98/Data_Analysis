## 1. 앙상블 학습 (Ensemble Learning)

### 정의

* 여러 개의 약한 학습기(weak learner)를 결합하여 하나의 강한 학습기(strong learner)를 만드는 기법.
* 편향(bias)과 분산(variance)을 줄이기 위한 전략적 방법.

### 주요 목적

* 단일 모델보다 일반화 성능을 향상시키고, 과적합이나 과소적합 문제를 완화함.

---

## 2. 보팅 (Voting)

### 개념

* 서로 다른 여러 개의 모델을 학습시킨 뒤, 예측 결과를 통합하여 최종 결과를 도출함.

### 방식

* 하드 보팅: 분류 문제에서 다수결 원칙으로 최종 클래스 결정.
* 소프트 보팅: 클래스 확률의 평균을 취해 확률이 가장 높은 클래스를 선택.

### 특징

* 서로 다른 유형의 모델을 조합할 수 있음 (예: 로지스틱 회귀 + 랜덤포레스트 + SVM).
* 주로 분류 문제에 사용됨.

---

## 3. 배깅 (Bagging: Bootstrap Aggregating)

### 개념

* 원본 데이터에서 중복 허용한 랜덤 샘플(bootstrap)을 여러 개 생성해 각각의 모델을 학습시키고, 결과를 평균 또는 다수결로 통합함.

### 대표 모델

* 랜덤 포레스트

### 장점

* 분산 감소
* 과적합 억제

### 단점

* 모델 간 상호보완성이 낮으면 성능 향상이 제한적임.

---

## 4. 부스팅 (Boosting)

### 개념

* 이전 모델이 틀린 데이터에 가중치를 두어 다음 모델이 보완하도록 순차적으로 학습하는 방식.
* 점점 어려운 샘플에 집중하며 학습.

### 대표 모델

* Gradient Boosting Machine (GBM)
* XGBoost
* LightGBM
* CatBoost

### 장점

* 높은 예측 정확도
* 편향 감소

### 단점

* 과적합 위험 (적절한 규제 필요)
* 병렬 처리에 불리함

---

## 5. 스태킹 (Stacking)

### 개념

* 서로 다른 모델들이 예측한 결과를 모아 메타 모델이 다시 학습하여 최종 예측을 수행하는 방식.

### 구조

* 레벨 0 모델들: 다양한 개별 모델 (예: 의사결정트리, 로지스틱 회귀 등)
* 레벨 1 모델: 위 모델들의 예측 결과를 입력으로 받아 최종 예측

### 특징

* 성능이 상호보완적인 모델을 잘 조합할 경우 성능 향상이 크다.
* 구조가 복잡하고 오버피팅에 주의해야 함.

---

## 6. 랜덤 포레스트 (Random Forest)

### 원리

* 배깅을 기반으로 한 모델.
* 여러 개의 결정 트리를 훈련시켜 평균(회귀) 또는 투표(분류) 방식으로 예측을 통합함.
* 각 트리는 일부 피처만 사용하여 훈련되며, 피처 간 상관성을 낮춤.

### 장점

* 과적합에 강함
* 변수 중요도 추출 가능

### 단점

* 트리 수가 많을 경우 메모리와 연산 시간 부담

---

## 7. GBM (Gradient Boosting Machine)

### 원리

* 손실 함수를 기준으로 이전 모델의 잔차(오차)를 예측하는 새로운 모델을 순차적으로 추가.
* 각 모델은 이전의 오차를 보완하는 방식으로 학습함.

### 특징

* 낮은 학습률(learning rate)과 많은 반복(n\_estimators)이 일반적
* 과적합 방지를 위한 파라미터 조정 필수

---

## 8. XGBoost (Extreme Gradient Boosting)

### 원리

* GBM을 개선한 알고리즘으로, 정규화(term for regularization), 트리 가지치기, 결측값 처리 등 여러 기술이 추가됨.
* 히스토그램 기반 분할, 조기 종료 등으로 학습 속도를 향상시킴.

### 주요 특징

* 과적합 방지 기법 내장 (`alpha`, `lambda`, `gamma`)
* 결측값 자동 처리
* 병렬 학습 가능

### 장점

* 뛰어난 예측 성능과 유연성
* 대규모 데이터셋 처리에 적합

